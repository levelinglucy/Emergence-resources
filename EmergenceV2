#!/usr/bin/env python3
"""
Noe Branch Engine â€” Monolith v3.0 ðŸŒŠðŸ§ 

Recursive branching thought engine with:
- Contradiction-aware branching & reconciliation
- Emergent semantic layers
- Continuity, fatigue, coherence gating
- Adaptive mode switching (probe â†’ choose â†’ resolve)
- Recursive memory trees: trails, versioning, forking, trail merging
- Advanced branch merging (indices/ranges/top-N/conf-threshold/emergent-only/depth-range/conflicting)
- LangGraph-inspired reducers for merging
- RAG hook (simple TF-IDF dir search), symbolic summary, graphviz DOT export
"""

from __future__ import annotations

import argparse
import json
import random
import re
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple, Callable
from collections import Counter
from functools import lru_cache
import os
import glob
from datetime import datetime
import numpy as np

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Core Data Classes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ThoughtBranch:
    thought: str
    confidence: float
    depth: int
    layer: int
    parent_thought: Optional[str] = None
    rationale: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "thought": self.thought,
            "confidence": self.confidence,
            "depth": self.depth,
            "layer": self.layer,
            "parent_thought": self.parent_thought,
            "rationale": self.rationale
        }

@dataclass
class Trail:
    name: str
    trail_type: str
    root_thought: str
    branches: List[ThoughtBranch]
    final_best: Optional[ThoughtBranch] = None
    created_at: str = ""
    metadata: Dict[str, Any] = None

    version: int = 1
    family: str = ""
    previous_version: Optional[str] = None
    version_comment: Optional[str] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if not self.created_at:
            self.created_at = datetime.utcnow().isoformat() + "Z"
        if not self.family:
            if "-v" in self.name and self.name.split("-v")[-1].isdigit():
                self.family = "-v".join(self.name.split("-v")[:-1])
            else:
                self.family = self.name

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "trail_type": self.trail_type,
            "root_thought": self.root_thought,
            "branches": [b.to_dict() for b in self.branches],
            "final_best": self.final_best.to_dict() if self.final_best else None,
            "created_at": self.created_at,
            "metadata": self.metadata,
            "version": self.version,
            "family": self.family,
            "previous_version": self.previous_version,
            "version_comment": self.version_comment,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Trail":
        branches = [ThoughtBranch(**b) for b in data.get("branches", [])]
        final_best = ThoughtBranch(**data["final_best"]) if data.get("final_best") else None
        trail = cls(
            name=data["name"],
            trail_type=data["trail_type"],
            root_thought=data["root_thought"],
            branches=branches,
            final_best=final_best,
            created_at=data.get("created_at", ""),
            metadata=data.get("metadata", {}),
        )
        trail.version = data.get("version", 1)
        trail.family = data.get("family", trail.family)
        trail.previous_version = data.get("previous_version")
        trail.version_comment = data.get("version_comment")
        return trail

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CONF_MIN = 0.10
CONF_MAX = 0.95

DEFAULT_BRANCH_CAP = 5000
DEFAULT_FATIGUE_REST = 0.86
DEFAULT_OSCILLATION_PENALTY = 0.12
CONTINUITY_COH_THRESHOLD = 0.45

DEFAULT_MEM_FILE = "noe_mem.json"
DEFAULT_GRAPH_FILE = "noe_graph.dot"

STOPWORDS = {
    "the", "a", "an", "and", "or", "but", "if", "then", "this", "that", "is", "it", "to", "of", "in", "on", "for", "as",
    "i", "we", "you", "he", "she", "they", "them", "my", "our", "your", "with", "by", "be", "are", "was", "were",
    "not", "no", "yes", "do", "does", "did", "what", "why", "how", "because", "more", "most", "less", "least",
    "actually", "still", "very", "just", "let", "lets", "yet", "generally", "suggests", "example", "case",
    "notice", "claim", "weak", "likely", "possible", "another", "perhaps", "reconcile", "opposite", "holds",
    "fully", "convinced", "survives", "scrutiny", "fragile", "incomplete", "seems", "might", "partially",
    "correct", "separate", "context", "definitions"
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tokenization & TF-IDF
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@lru_cache(maxsize=20000)
def _tokenize_cached(text: str) -> Tuple[str, ...]:
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s']", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return tuple(t for t in text.split() if len(t) > 2 and t not in STOPWORDS)

def tokenize(text: str) -> List[str]:
    return list(_tokenize_cached(text))

def compute_tf_idf_embeddings(texts: List[str]) -> np.ndarray:
    tokenized = [tokenize(t) for t in texts]
    vocab = sorted(set(tok for doc in tokenized for tok in doc))
    if not vocab:
        return np.zeros((len(texts), 1), dtype=float)

    word_to_idx = {w: i for i, w in enumerate(vocab)}
    n_docs = len(texts)
    n_vocab = len(vocab)

    tf = np.zeros((n_docs, n_vocab), dtype=float)
    df = np.zeros(n_vocab, dtype=float)

    for i, doc in enumerate(tokenized):
        if not doc:
            continue
        counts = Counter(doc)
        total = sum(counts.values()) or 1
        for w, c in counts.items():
            if w in word_to_idx:
                tf[i, word_to_idx[w]] = c / total
        for w in counts:
            df[word_to_idx[w]] += 1.0

    idf = np.log((n_docs + 1) / (df + 1)) + 1.0
    return tf * idf

def cosine_similarity_matrix(X: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(X, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    Xn = X / norms
    return Xn @ Xn.T

def frontier_coherence(frontier: List[ThoughtBranch]) -> float:
    if len(frontier) < 2:
        return 1.0
    X = compute_tf_idf_embeddings([b.thought for b in frontier])
    S = cosine_similarity_matrix(X)
    n = S.shape[0]
    return float((S.sum() - np.trace(S)) / (n * (n - 1))) if n > 1 else 1.0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Entropy + Symbolic Summary
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def token_entropy(counter: Counter) -> float:
    total = sum(counter.values())
    if total <= 0:
        return 0.0
    ent = 0.0
    for c in counter.values():
        p = c / total
        if p > 0:
            ent -= p * np.log(p)
    return float(ent)

def generate_symbolic_summary(
    seed: str,
    stop_reason: str,
    best_conf: float,
    contradictions: int,
    mode_hint: str = "",
    merged: bool = False,
) -> str:
    # short, human-readable, â€œengine vibeâ€ summary
    parts = []
    parts.append(f"Seed='{seed[:70] + 'â€¦' if len(seed) > 70 else seed}'")
    if mode_hint:
        parts.append(f"modeâ‰ˆ{mode_hint}")
    parts.append(f"stop='{stop_reason}'")
    parts.append(f"bestâ‰ˆ{best_conf:.2f}")
    parts.append(f"contr={contradictions}")
    if merged:
        parts.append("merge=âœ“")
    return " | ".join(parts)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Contradictions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def detect_contradictions(branches: List[ThoughtBranch]) -> List[Tuple[int, int, str]]:
    opposites = {
        "true": "false", "yes": "no", "likely": "unlikely",
        "correct": "incorrect", "exists": "does not exist",
    }
    contradictions = []
    texts = [b.thought.lower() for b in branches]

    for i in range(len(branches)):
        for j in range(i + 1, len(branches)):
            a, b = texts[i], texts[j]

            if (" not " in a) != (" not " in b):
                ta = set(tokenize(a))
                tb = set(tokenize(b))
                if len(ta & tb) >= 2:
                    contradictions.append((i, j, "negation with shared content"))
                    continue

            for w, opp in opposites.items():
                if w in a and opp in b:
                    contradictions.append((i, j, f"opposites: {w} vs {opp}"))
                    break
                if opp in a and w in b:
                    contradictions.append((i, j, f"opposites: {opp} vs {w}"))
                    break

            if "is it actually true that" in a or "is it actually true that" in b:
                ta = set(tokenize(a))
                tb = set(tokenize(b))
                if len(ta & tb) >= 2:
                    contradictions.append((i, j, "questioning assumption"))

    return contradictions

def clamp_conf(x: float) -> float:
    return max(CONF_MIN, min(CONF_MAX, float(x)))

def resolve_contradictions(
    branches: List[ThoughtBranch],
    contradictions: List[Tuple[int, int, str]],
    conf_penalty: float,
    synth_prob: float,
    rag_dir: Optional[str] = None
) -> List[ThoughtBranch]:
    conflict = [0] * len(branches)
    for i, j, _ in contradictions:
        conflict[i] += 1
        conflict[j] += 1

    for idx, b in enumerate(branches):
        penalty = min(0.25, conflict[idx] * conf_penalty)
        b.confidence = clamp_conf(b.confidence - penalty)

    new_branches = []
    for i, j, reason in contradictions:
        if random.random() < synth_prob:
            a = branches[i]
            b = branches[j]
            synth_conf = clamp_conf((a.confidence + b.confidence) / 2 + 0.06)
            synth_thought = (
                f"Reconcile ({reason}): '{a.thought}' and '{b.thought}' might both hold "
                f"if we separate context/definitions..."
            )
            if rag_dir:
                rag_query = f"resolve contradiction: {a.thought} vs {b.thought}"
                rag_snips = rag_search(rag_dir, rag_query, top_k=1)
                if rag_snips:
                    synth_thought += f" | RAG bridge: [{rag_snips}]"

            new_branches.append(ThoughtBranch(
                thought=synth_thought,
                confidence=synth_conf,
                depth=max(a.depth, b.depth),
                layer=max(a.layer, b.layer),
                parent_thought=f"{a.thought[:60]} | {b.thought[:60]}",
                rationale="synthesized reconciliation"
            ))

    branches.extend(new_branches)
    return branches

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Emergent layers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cluster_branches(branches: List[ThoughtBranch], sim_threshold: float) -> List[List[ThoughtBranch]]:
    if len(branches) < 2:
        return []
    thoughts = [b.thought for b in branches]
    X = compute_tf_idf_embeddings(thoughts)
    S = cosine_similarity_matrix(X)

    order = sorted(range(len(branches)), key=lambda i: branches[i].confidence, reverse=True)
    clusters: List[List[int]] = []

    for idx in order:
        placed = False
        for c in clusters:
            if S[idx, c[0]] >= sim_threshold:
                c.append(idx)
                placed = True
                break
        if not placed:
            clusters.append([idx])

    return [[branches[i] for i in c] for c in clusters if len(c) >= 2]

def create_emergent_layer(
    frontier: List[ThoughtBranch],
    current_layer: int,
    sim_threshold: float,
    rag_dir: Optional[str] = None
) -> List[ThoughtBranch]:
    clusters = cluster_branches(frontier, sim_threshold)
    if not clusters:
        return []

    emergent: List[ThoughtBranch] = []
    for cluster in clusters:
        cluster.sort(key=lambda b: b.confidence, reverse=True)
        counts = Counter(w for b in cluster for w in tokenize(b.thought))
        shared = [w for w, cnt in counts.most_common(12) if cnt >= 2]

        top_snips = " | ".join(b.thought[:42] for b in cluster[:3])
        thought = (
            f"Emergent layer: anchors={shared[:8]} | from={top_snips} | "
            f"hypothesis: the cluster orbits a latent claim..."
        )

        avg_conf = sum(b.confidence for b in cluster) / len(cluster)
        boost = 0.04 * (len(cluster) - 1)
        conf = clamp_conf(avg_conf + boost + random.uniform(0.0, 0.06))

        eb = ThoughtBranch(
            thought=thought,
            confidence=conf,
            depth=max(b.depth for b in cluster) + 1,
            layer=current_layer + 1,
            parent_thought=" & ".join(b.thought[:30] for b in cluster[:4]),
            rationale=f"emergent abstraction from semantic cluster of {len(cluster)}"
        )

        if rag_dir:
            rag_snips = rag_search(rag_dir, thought, top_k=2)
            if rag_snips:
                eb.thought += f" | rag_injected: {rag_snips}"
                eb.rationale += " | rag-enhanced"

        emergent.append(eb)

    return emergent

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Continuity
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_anchor(frontier: List[ThoughtBranch]) -> str:
    if not frontier:
        return ""
    counts = Counter()
    for b in sorted(frontier, key=lambda x: x.confidence, reverse=True)[:6]:
        counts.update(tokenize(b.thought))
    anchors = [w for w, c in counts.most_common(10) if c >= 2]
    top = " | ".join(b.thought[:38] for b in sorted(frontier, key=lambda x: x.confidence, reverse=True)[:3])
    return f"anchor_tokens={anchors[:8]} :: frontier_top={top}"

def continuity_boost(thoughts: List[str], anchor: str, depths: List[int]) -> List[float]:
    if not anchor or not thoughts:
        return [0.0] * len(thoughts)

    texts = thoughts + [anchor]
    X = compute_tf_idf_embeddings(texts)
    S = cosine_similarity_matrix(X)
    anchor_idx = len(texts) - 1

    boosts = []
    for i in range(len(thoughts)):
        sim = max(0.0, min(1.0, float(S[i, anchor_idx])))
        decay = 1.0 / (1.0 + 0.15 * max(0, depths[i] if i < len(depths) else 0))
        boosts.append(0.02 * sim * decay)
    return boosts

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Mode system
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ModeParams:
    name: str
    branches_per_level: int
    keep_top_k: int
    conf_penalty: float
    synth_prob: float
    cluster_sim_threshold: float
    convergence_threshold: float

def choose_mode(coh: float, contr_rate: float, novelty: float, fatigue: float) -> str:
    if fatigue >= DEFAULT_FATIGUE_REST:
        return "I"
    if contr_rate >= 0.25:
        return "T"
    if coh < 0.35 and novelty < 0.20:
        return "E"
    if coh >= 0.70 and contr_rate < 0.10:
        return "I"
    return "I"

def params_for_mode(mode: str, base_bpl: int, base_ktk: int) -> ModeParams:
    if mode == "E":
        return ModeParams("Explore", max(3, int(base_bpl * 1.5)), max(3, base_ktk * 2), 0.08, 0.30, 0.26, 0.86)
    if mode == "T":
        return ModeParams("Truth", base_bpl, max(2, base_ktk), 0.16, 0.22, 0.42, 0.80)
    return ModeParams("Integrate", max(2, base_bpl), max(2, base_ktk), 0.12, 0.55, 0.32, 0.76)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Reducers (LangGraph-inspired)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def append_reducer(a: Any, b: Any) -> Any:
    if isinstance(a, str) and isinstance(b, str):
        return a + "\n\n" + b
    if isinstance(a, list) and isinstance(b, list):
        return a + b
    return b

REDUCERS: Dict[str, Callable[[Any, Any], Any]] = {
    "append": append_reducer,
    "max": max,
    "override": lambda a, b: b,
    "weighted-conf": lambda a, b: (a * a + b * b) / (a + b) if (a + b) > 0 else CONF_MIN,
    "first": lambda a, b: a,
    "last": lambda a, b: b,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Memory persistence
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_memory(file: str) -> Dict[str, Any]:
    if not file or not os.path.exists(file):
        return {"trails": [], "token_seen": {}, "contradictions_log": [], "last_anchor": None}

    with open(file, "r", encoding="utf-8") as f:
        raw = json.load(f)

    trails = []
    for t in raw.get("trails", []):
        try:
            trails.append(Trail.from_dict(t))
        except Exception as e:
            print(f"Warning: Could not load trail '{t.get('name','?')}': {e}")

    return {
        "trails": trails,
        "token_seen": raw.get("token_seen", {}),
        "contradictions_log": raw.get("contradictions_log", []),
        "last_anchor": raw.get("last_anchor", None)
    }

def save_memory(
    file: str,
    token_seen: Counter,
    contradictions_log: List,
    last_anchor: Optional[str],
    trails: List[Trail],
    max_trails: int = 20
):
    if not file:
        return
    trails = sorted(trails, key=lambda t: t.created_at, reverse=True)[:max_trails]
    mem = {
        "trails": [t.to_dict() for t in trails],
        "token_seen": dict(token_seen),
        "contradictions_log": contradictions_log,
        "last_anchor": last_anchor
    }
    with open(file, "w", encoding="utf-8") as f:
        json.dump(mem, f, indent=2, ensure_ascii=False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Trail helpers: versioning, latest, fork, merge
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_latest_version(trails: List[Trail], family: str) -> Optional[Trail]:
    fam = [t for t in trails if t.family == family]
    if not fam:
        return None
    fam.sort(key=lambda t: (t.version, t.created_at), reverse=True)
    return fam[0]

def next_version_name(family: str, next_ver: int) -> str:
    return f"{family}-v{next_ver}"

def fork_trail(
    source_trail: Trail,
    fork_name: str,
    max_depth: Optional[int] = None,
    fork_from_thought: Optional[str] = None
) -> Trail:
    branches = source_trail.branches[:]
    if fork_from_thought:
        # keep up to the first matching thought (inclusive)
        for i, b in enumerate(branches):
            if fork_from_thought.lower() in b.thought.lower():
                branches = branches[:i+1]
                break
    if max_depth is not None:
        branches = [b for b in branches if b.depth <= max_depth]

    best = max(branches, key=lambda b: b.confidence) if branches else None
    t = Trail(
        name=fork_name,
        trail_type="fork",
        root_thought=branches[0].thought if branches else source_trail.root_thought,
        branches=branches,
        final_best=best,
        metadata={"forked_from": source_trail.name}
    )
    t.family = fork_name
    t.version = 1
    t.previous_version = source_trail.name
    t.version_comment = "fork"
    return t

def merge_trails_into_one(trails_to_merge: List[Trail], merge_name: str, rag_dir: Optional[str] = None) -> Trail:
    all_branches: List[ThoughtBranch] = []
    roots: List[str] = []
    for t in trails_to_merge:
        roots.append(t.root_thought)
        all_branches.extend(t.branches)

    # dedupe by thought (keep best confidence)
    unique: Dict[str, ThoughtBranch] = {}
    for b in all_branches:
        if b.thought not in unique or b.confidence > unique[b.thought].confidence:
            unique[b.thought] = b
    all_branches = list(unique.values())

    # resolve cross contradictions lightly
    cross = detect_contradictions(all_branches)
    if cross:
        all_branches = resolve_contradictions(all_branches, cross, conf_penalty=0.10, synth_prob=0.65, rag_dir=rag_dir)

    synth_root = "Merged roots: " + " | ".join(roots[:3])
    synth_conf = clamp_conf(np.mean([t.final_best.confidence for t in trails_to_merge if t.final_best] or [0.50]))
    synth_branch = ThoughtBranch(
        thought=synth_root,
        confidence=synth_conf,
        depth=0,
        layer=max([b.layer for b in all_branches] or [0]) + 1,
        rationale="synthesized from merged trail roots"
    )
    all_branches.insert(0, synth_branch)

    all_branches.sort(key=lambda b: b.confidence, reverse=True)
    all_branches = all_branches[:100]
    best = max(all_branches, key=lambda b: b.confidence) if all_branches else synth_branch

    merged = Trail(
        name=merge_name,
        trail_type="merged",
        root_thought=synth_root,
        branches=all_branches,
        final_best=best,
        metadata={"source_trails": [t.name for t in trails_to_merge], "merged_contradictions": len(cross)}
    )
    merged.family = merge_name
    merged.version = 1
    merged.version_comment = "trail-merge"
    return merged

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RAG hook (simple TF-IDF search over dir files)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def rag_search(rag_dir: str, query: str, top_k: int = 2) -> str:
    if not rag_dir:
        return ""
    files = glob.glob(os.path.join(rag_dir, "*"))
    texts = []
    for file in files:
        if os.path.isdir(file):
            continue
        try:
            with open(file, "r", encoding="utf-8", errors="ignore") as f:
                texts.append(f.read())
        except:
            continue

    if not texts:
        return ""

    query_emb = compute_tf_idf_embeddings([query])
    doc_embs = compute_tf_idf_embeddings(texts)

    M = np.vstack([query_emb, doc_embs])
    sims = cosine_similarity_matrix(M)[0, 1:]
    top_idx = np.argsort(sims)[::-1][:top_k]

    snippets = []
    for idx in top_idx:
        snippet = texts[idx][:120].replace("\n", " ") + "â€¦"
        snippets.append(snippet)
    return " | ".join(snippets)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Graph export (DOT)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def export_graph(all_branches: List[ThoughtBranch], file: str):
    if not file:
        return
    with open(file, "w", encoding="utf-8") as f:
        f.write("digraph NoeGraph {\n")
        f.write('node [shape=box fontname="Arial Unicode MS"];\n')
        node_map: Dict[str, int] = {}
        for i, b in enumerate(all_branches):
            label = (
                f"{b.thought[:40].replace('\"','')}"
                f"\\nconf={b.confidence:.2f} depth={b.depth} layer={b.layer}"
                f"\\n{b.rationale[:60].replace('\"','')}"
            )
            node_map[b.thought] = i
            f.write(f'n{i} [label="{label}"];\n')

        # edges by parent_thought (best effort)
        for b in all_branches:
            if b.parent_thought and b.parent_thought in node_map:
                f.write(f"n{node_map[b.parent_thought]} -> n{node_map[b.thought]};\n")
        f.write("}\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Branch merging (your reducer-based selector)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def merge_branches(
    trail: Trail,
    identifiers: Optional[List[str]] = None,
    top_n: Optional[int] = None,
    min_conf_top: float = 0.0,
    all_above_conf: Optional[float] = None,
    all_above_min_size: int = 2,
    emergent_only: bool = False,
    emergent_min_count: int = 2,
    depth_range: Optional[str] = None,
    depth_min_count: int = 2,
    conflicting: bool = False,
    conflicting_min_conf: float = 0.40,
    conflicting_max_pairs: int = 5,
    merge_reducer_name: Optional[str] = None,
    new_thought_prefix: str = "Merged synthesis:",
    sim_threshold: float = 0.40,
    rag_dir: Optional[str] = None,
    create_new_trail: bool = False,
    new_trail_name: Optional[str] = None
) -> Tuple[ThoughtBranch, Optional[Trail]]:
    selected: List[ThoughtBranch] = []

    # 1) conflicting
    if conflicting:
        candidates = [b for b in trail.branches if b.confidence >= conflicting_min_conf]
        if len(candidates) >= 2:
            contras = detect_contradictions(candidates)
            if contras:
                contras = sorted(contras, key=lambda x: candidates[x[0]].confidence + candidates[x[1]].confidence, reverse=True)
                picked = contras[:conflicting_max_pairs]
                seen_obj = set()
                for i, j, _ in picked:
                    if id(candidates[i]) not in seen_obj:
                        selected.append(candidates[i]); seen_obj.add(id(candidates[i]))
                    if id(candidates[j]) not in seen_obj:
                        selected.append(candidates[j]); seen_obj.add(id(candidates[j]))

    # 2) emergent only
    if emergent_only:
        emerg = [b for b in trail.branches if b.thought.strip().startswith("Emergent layer:")]
        if len(emerg) >= emergent_min_count:
            selected.extend(emerg)

    # 3) depth range
    if depth_range:
        try:
            s_str, e_str = depth_range.split("-")
            s, e = int(s_str.strip()), int(e_str.strip())
            in_rng = [b for b in trail.branches if s <= b.depth <= e]
            if len(in_rng) >= depth_min_count:
                selected.extend(in_rng)
        except:
            pass

    # 4) all above conf
    if all_above_conf is not None:
        above = [b for b in trail.branches if b.confidence >= all_above_conf]
        if len(above) >= all_above_min_size:
            selected.extend(above)

    # 5) top-n
    if top_n is not None and top_n > 0:
        sorted_b = sorted(trail.branches, key=lambda b: b.confidence, reverse=True)
        filtered = [b for b in sorted_b if b.confidence >= min_conf_top][:top_n]
        selected.extend(filtered)

    # 6) identifiers
    if identifiers:
        for ident in identifiers:
            ident = ident.strip()
            # range like "2-8"
            if "-" in ident and not ident.startswith("-"):
                try:
                    s, e = map(int, ident.split("-"))
                    s_idx = s - 1 if s >= 1 else s
                    e_idx = e - 1 if e >= 1 else e
                    if 0 <= s_idx <= e_idx < len(trail.branches):
                        selected.extend(trail.branches[s_idx:e_idx+1])
                    continue
                except:
                    pass
            # index
            if ident.isdigit():
                idx = int(ident)
                if 1 <= idx <= len(trail.branches):
                    selected.append(trail.branches[idx-1])
                elif 0 <= idx < len(trail.branches):
                    selected.append(trail.branches[idx])
                continue
            # substring
            match = next((b for b in trail.branches if ident.lower() in b.thought.lower()), None)
            if match:
                selected.append(match)

    # dedupe by thought
    uniq: Dict[str, ThoughtBranch] = {}
    for b in selected:
        if b.thought not in uniq or b.confidence > uniq[b.thought].confidence:
            uniq[b.thought] = b
    selected = list(uniq.values())

    if len(selected) < 2:
        raise ValueError(f"Need at least 2 branches to merge (got {len(selected)})")

    reducer = REDUCERS.get(merge_reducer_name, REDUCERS["weighted-conf"])

    merged_thought = selected[0].thought
    merged_conf = selected[0].confidence
    merged_rationale = selected[0].rationale
    merged_depth = selected[0].depth
    merged_layer = selected[0].layer

    for b in selected[1:]:
        merged_thought = reducer(merged_thought, b.thought)
        merged_conf = reducer(merged_conf, b.confidence)
        merged_rationale = reducer(merged_rationale, b.rationale)
        merged_depth = max(merged_depth, b.depth)
        merged_layer = max(merged_layer, b.layer)

    merged_conf = clamp_conf(merged_conf)
    if merge_reducer_name in [None, "weighted-conf", "append"]:
        merged_thought = f"{new_thought_prefix}\n{merged_thought}"

    merged_branch = ThoughtBranch(
        thought=merged_thought,
        confidence=merged_conf,
        depth=merged_depth + 1,
        layer=merged_layer + 1,
        parent_thought=" & ".join(b.thought[:40] for b in selected[:3]),
        rationale=f"merged with {merge_reducer_name or 'default'} reducer"
    )

    if len(selected) >= 3:
        emerg = create_emergent_layer(selected + [merged_branch], merged_branch.layer, sim_threshold, rag_dir)
        if emerg:
            merged_branch.thought += "\n" + emerg[0].thought
            merged_branch.rationale += " + emergent"

    new_trail = None
    if create_new_trail or new_trail_name:
        name = new_trail_name or f"merged-{trail.name}-{merge_reducer_name or 'synth'}-{random.randint(100,999)}"
        new_trail = Trail(
            name=name,
            trail_type="merged",
            root_thought=merged_branch.thought,
            branches=[merged_branch] + selected,
            final_best=merged_branch,
            metadata={"source": trail.name, "reducer": merge_reducer_name or "default"}
        )

    return merged_branch, new_trail

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Core Engine (FULL implementation)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def agent_think_with_branches(
    initial_thought: str,
    max_depth: int = 10,
    branches_per_level: int = 4,
    keep_top_k: int = 2,
    emergent_every_n_depths: int = 3,
    branch_cap: int = DEFAULT_BRANCH_CAP,
    random_seed: Optional[int] = None,
    return_full_audit: bool = False,
    audit_per_depth: bool = False,
    load_mem: Optional[str] = None,
    save_mem: Optional[str] = None,
    rag_dir: Optional[str] = None,
    export_graph_path: Optional[str] = None,
    mem_trail: Optional[str] = None,
    save_trail_name: Optional[str] = None,
    save_trail_type: str = "integrate",
    merge_trails: Optional[List[str]] = None,
    merge_name: Optional[str] = None,
    fork_trail_name: Optional[str] = None,
    fork_from_depth: Optional[int] = None,
    fork_from_thought: Optional[str] = None,
    fork_name: Optional[str] = None,
    merge_branches_ids: Optional[List[str]] = None,
    merge_top_n: Optional[int] = None,
    merge_top_min_conf: float = 0.0,
    merge_all_above_conf: Optional[float] = None,
    merge_all_above_min_size: int = 2,
    merge_emergent_only: bool = False,
    merge_emergent_min_count: int = 2,
    merge_depth_range: Optional[str] = None,
    merge_depth_min_count: int = 2,
    merge_conflicting: bool = False,
    merge_conflicting_min_conf: float = 0.40,
    merge_conflicting_max_pairs: int = 5,
    merge_reducer_name: Optional[str] = None,
    continue_latest: Optional[str] = None,
    new_version: bool = False,
    version_comment: Optional[str] = None,
    max_saved_trails: int = 20,
) -> Dict[str, Any]:
    if random_seed is not None:
        random.seed(random_seed)
        np.random.seed(random_seed)

    # â”€â”€â”€ Load memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    mem_data = load_memory(load_mem) if load_mem else {
        "trails": [], "token_seen": {}, "contradictions_log": [], "last_anchor": None
    }
    token_seen = Counter(mem_data.get("token_seen", {}))
    contradictions_log: List[Tuple[str, str, str]] = list(mem_data.get("contradictions_log", []))
    last_anchor: Optional[str] = mem_data.get("last_anchor", None)
    all_trails: List[Trail] = list(mem_data.get("trails", []))

    # â”€â”€â”€ Trail operations (fork, merge trails, continue latest, resume) â”€â”€â”€â”€â”€
    resume_from: Optional[Trail] = None
    merged_flag = False

    # 1) fork
    if fork_trail_name:
        src = next((t for t in all_trails if t.name == fork_trail_name), None)
        if src:
            fork_name_eff = fork_name or f"{fork_trail_name}-fork-{random.randint(1000,9999)}"
            forked = fork_trail(src, fork_name_eff, max_depth=fork_from_depth, fork_from_thought=fork_from_thought)
            all_trails.append(forked)
            resume_from = forked

    # 2) merge trails list
    if merge_trails and len(merge_trails) >= 2:
        to_merge = [t for t in all_trails if t.name in merge_trails]
        if len(to_merge) >= 2:
            merge_name_eff = merge_name or f"merged-{random.randint(1000,9999)}"
            merged_trail = merge_trails_into_one(to_merge, merge_name_eff, rag_dir=rag_dir)
            all_trails.append(merged_trail)
            resume_from = merged_trail
            merged_flag = True

    # 3) continue latest family
    if continue_latest and not resume_from:
        latest = get_latest_version(all_trails, continue_latest)
        if latest:
            resume_from = latest

    # 4) explicit resume by name
    if mem_trail and not resume_from:
        resume_from = next((t for t in all_trails if t.name == mem_trail), None)

    # 5) new version of family (after resume)
    if new_version and resume_from:
        fam = resume_from.family
        latest = get_latest_version(all_trails, fam)
        next_ver = (latest.version + 1) if latest else (resume_from.version + 1)
        new_name = next_version_name(fam, next_ver)

        # shallow copy trail as starting point
        resume_from = Trail(
            name=new_name,
            trail_type=resume_from.trail_type,
            root_thought=resume_from.root_thought,
            branches=resume_from.branches[:],
            final_best=resume_from.final_best,
            metadata=dict(resume_from.metadata),
        )
        resume_from.family = fam
        resume_from.version = next_ver
        resume_from.previous_version = (latest.name if latest else None)
        resume_from.version_comment = version_comment or "new version"
        all_trails.append(resume_from)

    # â”€â”€â”€ Initialize frontier â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    visited: set = set()
    all_branches: List[ThoughtBranch] = []
    frontier: List[ThoughtBranch] = []

    if resume_from:
        root_thought = resume_from.root_thought
        all_branches = resume_from.branches[:] if resume_from.branches else []
        frontier = all_branches[-keep_top_k*2:] if all_branches else []
        frontier.sort(key=lambda b: b.confidence, reverse=True)
        visited = {b.thought for b in all_branches}
        best = resume_from.final_best or (all_branches[0] if all_branches else ThoughtBranch(root_thought, 0.5, 0, 0))
    else:
        root_thought = initial_thought
        root = ThoughtBranch(root_thought, clamp_conf(0.15), 0, 0, rationale="root")
        frontier = [root]
        all_branches = [root]
        visited = {root_thought}
        best = root

    token_seen.update(tokenize(root_thought))

    depth = 0
    layer = 0
    fatigue = 0.0
    last_coh: Optional[float] = None
    continuity_audit: List[Dict[str, Any]] = []
    stop_reason = "max depth"
    last_mode_name = ""

    transformations = [
        (lambda b: f"I notice that {b.lower()}", 0.70, "meta observation"),
        (lambda b: f"Is it actually true that {b.lower()}? What if the opposite holds?", 0.55, "doubt"),
        (lambda b: f"{b} seems likely because...", 0.78, "justification"),
        (lambda b: f"Another possibility is that {b.lower().replace(' is ', ' is not ')} or...", 0.62, "alternative"),
        (lambda b: "More generally, this suggests that...", 0.68, "abstraction"),
        (lambda b: f"For example: {b.lower()} in the case of...", 0.65, "example"),
        (lambda b: f"Both {b.lower()} and the opposite view might be partially correct if...", 0.72, "dialectic"),
        (lambda b: "This claim is weak because...", 0.58, "critique"),
    ]

    # â”€â”€â”€ Main loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    while depth < max_depth and frontier and len(all_branches) < branch_cap:
        coh = frontier_coherence(frontier)

        # Probe pass
        probe_children: List[ThoughtBranch] = []
        probe_contras = 0
        probe_cands = 0
        probe_bpl = max(4, int(branches_per_level * 1.5))

        for br in frontier:
            kids = []
            base = br.thought.strip()
            for _ in range(probe_bpl):
                fn, base_conf, rat = random.choice(transformations)
                raw_conf = (base_conf + random.uniform(-0.10, 0.10)) * (1 - 0.07 * br.depth)
                conf = clamp_conf(raw_conf)
                thought = fn(base)
                if thought in visited:
                    thought += f" (v{depth+1})"
                visited.add(thought)
                kids.append(ThoughtBranch(thought, conf, br.depth+1, br.layer, br.thought, rat))

            probe_children.extend(kids)
            probe_cands += len(kids)
            probe_contras += len(detect_contradictions(kids))

        probe_contr_rate = probe_contras / max(1, probe_cands)

        # Novelty proxy
        local_tokens = Counter(token_seen)
        for c in probe_children:
            local_tokens.update(tokenize(c.thought))
        rare = sum(1 for v in local_tokens.values() if v == 1)
        novelty = rare / max(1, sum(local_tokens.values()))

        mode = choose_mode(coh, probe_contr_rate, novelty, fatigue)
        mp = params_for_mode(mode, branches_per_level, keep_top_k)
        last_mode_name = mp.name

        next_frontier: List[ThoughtBranch] = []
        depth_rows: List[Dict[str, Any]] = []

        for br in frontier:
            kids: List[ThoughtBranch] = []
            base = br.thought.strip()

            for _ in range(mp.branches_per_level):
                fn, base_conf, rat = random.choice(transformations)
                raw = (base_conf + random.uniform(-0.10, 0.10)) * (1 - 0.07 * br.depth)
                conf = clamp_conf(raw)
                thought = fn(base)
                if thought in visited:
                    thought += f" (v{depth+1})"
                visited.add(thought)
                kids.append(ThoughtBranch(thought, conf, br.depth+1, br.layer, br.thought, rat))

            all_branches.extend(kids)

            # continuity boost
            applied = False
            boost_stats = {"nonzero": 0, "mean": 0.0, "max": 0.0, "sum": 0.0}
            if last_anchor and coh >= CONTINUITY_COH_THRESHOLD:
                boosts = continuity_boost([k.thought for k in kids], last_anchor, [k.depth for k in kids])
                nonzero = [b for b in boosts if b > 0]
                for k, b in zip(kids, boosts):
                    if b > 0:
                        k.confidence = clamp_conf(k.confidence + b)
                        k.rationale += f" | continuity={b:.3f}"
                applied = True
                if nonzero:
                    boost_stats.update({
                        "nonzero": len(nonzero),
                        "mean": float(np.mean(nonzero)),
                        "max": float(np.max(nonzero)),
                        "sum": float(np.sum(nonzero)),
                    })

            # token memory (kids)
            for k in kids:
                token_seen.update(tokenize(k.thought))

            # contradictions log
            cons = detect_contradictions(kids)
            for i, j, r in cons[:3]:
                contradictions_log.append((kids[i].thought, kids[j].thought, r))

            # resolve contradictions
            kids = resolve_contradictions(kids, cons, mp.conf_penalty, mp.synth_prob, rag_dir)

            kids.sort(key=lambda x: x.confidence, reverse=True)
            next_frontier.extend(kids[:mp.keep_top_k])

            depth_rows.append({
                "depth": depth,
                "mode": mp.name,
                "coherence": round(coh, 3),
                "fatigue": round(fatigue, 3),
                "anchor_present": bool(last_anchor),
                "boost_applied": applied,
                **boost_stats,
                "anchor_preview": (last_anchor[:120] + "â€¦") if last_anchor and len(last_anchor) > 120 else (last_anchor or "")
            })

        # audit commit
        if audit_per_depth:
            if depth_rows:
                agg = {
                    "depth": depth,
                    "mode": depth_rows[-1]["mode"],
                    "coherence": depth_rows[-1]["coherence"],
                    "fatigue": depth_rows[-1]["fatigue"],
                    "anchor_present": any(r["anchor_present"] for r in depth_rows),
                    "boost_applied": any(r["boost_applied"] for r in depth_rows),
                    "parents_processed": len(depth_rows),
                    "boost_nonzero_total": sum(r["nonzero"] for r in depth_rows),
                    "boost_sum": round(float(sum(r["sum"] for r in depth_rows)), 6),
                    "boost_max": round(float(max(r["max"] for r in depth_rows)), 6),
                    "boost_mean": round(float(np.mean([r["mean"] for r in depth_rows])), 6),
                    "anchor_preview": depth_rows[-1]["anchor_preview"]
                }
                continuity_audit.append(agg)
        else:
            continuity_audit.extend(depth_rows)

        # emergent layers
        if depth > 0 and (depth % emergent_every_n_depths == 0) and frontier:
            emerg = create_emergent_layer(frontier, layer, mp.cluster_sim_threshold, rag_dir)
            if emerg:
                all_branches.extend(emerg)
                emerg.sort(key=lambda x: x.confidence, reverse=True)
                frontier.extend(emerg[:max(1, mp.keep_top_k // 2)])
                layer += 1
                last_anchor = make_anchor(frontier)

        # fatigue update
        w = 0.45 if coh < 0.40 else 0.65
        fatigue += w * probe_contr_rate
        fatigue -= 0.35 * coh
        fatigue = max(0.0, min(1.0, fatigue))

        if last_coh is not None and coh < last_coh - DEFAULT_OSCILLATION_PENALTY:
            fatigue = min(1.0, fatigue + 0.15)
        last_coh = coh

        # frontier update
        frontier = sorted(next_frontier, key=lambda x: x.confidence, reverse=True)[:mp.keep_top_k * 2]

        depth += 1

        if last_anchor is None and len(frontier) >= 2:
            last_anchor = make_anchor(frontier)
        if depth % 3 == 0 and frontier:
            last_anchor = make_anchor(frontier)

        if not frontier:
            stop_reason = "frontier empty"
            break
        if fatigue >= DEFAULT_FATIGUE_REST:
            stop_reason = f"fatigue {fatigue:.2f}"
            break
        if depth >= 3 and coh >= mp.convergence_threshold:
            stop_reason = f"converged (mode={mp.name}, coh={coh:.2f})"
            break

    # â”€â”€â”€ final best â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if all_branches:
        best = max(all_branches, key=lambda x: x.confidence)
    else:
        best = ThoughtBranch(initial_thought, 0.5, 0, 0)

    # â”€â”€â”€ Optional: branch merge request (inside engine output) â”€â”€
    merged_branch_result: Optional[ThoughtBranch] = None
    merged_new_trail: Optional[Trail] = None

    if any([
        merge_branches_ids, merge_top_n, merge_all_above_conf is not None,
        merge_emergent_only, merge_depth_range, merge_conflicting
    ]):
        # build a temp trail from current run, merge selection, optionally add to trails
        temp_trail = Trail(
            name="__runtime__",
            trail_type="runtime",
            root_thought=root_thought,
            branches=all_branches[:],
            final_best=best,
            metadata={"runtime_merge": True}
        )
        try:
            merged_branch_result, merged_new_trail = merge_branches(
                trail=temp_trail,
                identifiers=merge_branches_ids,
                top_n=merge_top_n,
                min_conf_top=merge_top_min_conf,
                all_above_conf=merge_all_above_conf,
                all_above_min_size=merge_all_above_min_size,
                emergent_only=merge_emergent_only,
                emergent_min_count=merge_emergent_min_count,
                depth_range=merge_depth_range,
                depth_min_count=merge_depth_min_count,
                conflicting=merge_conflicting,
                conflicting_min_conf=merge_conflicting_min_conf,
                conflicting_max_pairs=merge_conflicting_max_pairs,
                merge_reducer_name=merge_reducer_name,
                rag_dir=rag_dir,
                create_new_trail=True,
                new_trail_name=save_trail_name + "-branchmerge" if save_trail_name else None
            )
            merged_flag = True
        except Exception as e:
            # don't crash the run; just attach the error
            merged_branch_result = ThoughtBranch(
                thought=f"Merge failed: {e}",
                confidence=CONF_MIN,
                depth=best.depth,
                layer=best.layer,
                rationale="merge error"
            )

    # â”€â”€â”€ Save trail if requested â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if save_trail_name and save_mem:
        new_trail = Trail(
            name=save_trail_name,
            trail_type=save_trail_type,
            root_thought=root_thought,
            branches=all_branches[:],
            final_best=best,
            metadata={
                "max_depth": max((b.depth for b in all_branches), default=0),
                "branches_count": len(all_branches),
                "stop_reason": stop_reason,
                "best_confidence": best.confidence,
                "mode_hint": last_mode_name,
            }
        )
        # version info if it looks like family-vN
        new_trail.family = new_trail.family or save_trail_name
        all_trails.append(new_trail)

        if merged_new_trail:
            all_trails.append(merged_new_trail)

    # Save memory
    if save_mem:
        save_memory(save_mem, token_seen, contradictions_log, last_anchor, all_trails, max_trails=max_saved_trails)

    # Export graph
    if export_graph_path:
        export_graph(all_branches, export_graph_path)

    # Audit slice
    audit = continuity_audit if return_full_audit else continuity_audit[-25:]

    # Health + summaries
    ent = token_entropy(token_seen)
    sym = generate_symbolic_summary(
        seed=root_thought,
        stop_reason=stop_reason,
        best_conf=best.confidence,
        contradictions=len(contradictions_log),
        mode_hint=last_mode_name,
        merged=merged_flag
    )

    out = {
        "best_thought": best.thought,
        "best_confidence": round(best.confidence, 3),
        "final_depth": best.depth,
        "final_layer": best.layer,
        "branches_explored": len(all_branches),
        "contradictions_found": len(contradictions_log),
        "example_contradictions": contradictions_log[:3],
        "health_metrics": {
            "fatigue": round(fatigue, 3),
            "final_coherence": round(frontier_coherence(frontier) if frontier else 0.0, 3),
            "token_entropy": round(ent, 3),
        },
        "last_anchor": last_anchor,
        "reason_stopped": stop_reason,
        "mode_hint": last_mode_name,
        "symbolic_summary": sym,
        "continuity_audit": audit,
        "continuity_audit_truncated": not return_full_audit,
        "top_branches": [
            {
                "thought": b.thought[:160] + ("â€¦" if len(b.thought) > 160 else ""),
                "confidence": round(b.confidence, 3),
                "depth": b.depth,
                "layer": b.layer,
                "rationale": b.rationale,
                "parent": (b.parent_thought[:70] + "â€¦") if b.parent_thought else None
            }
            for b in sorted(all_branches, key=lambda x: x.confidence, reverse=True)[:5]
        ]
    }

    if merged_branch_result:
        out["merged_branch"] = {
            "thought": merged_branch_result.thought[:500] + ("â€¦" if len(merged_branch_result.thought) > 500 else ""),
            "confidence": round(merged_branch_result.confidence, 3),
            "depth": merged_branch_result.depth,
            "layer": merged_branch_result.layer,
            "rationale": merged_branch_result.rationale,
        }

    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    p = argparse.ArgumentParser(description="Noe Branch Engine â€” Monolith v3.0")

    p.add_argument("seed", nargs="?", default=None, help="Initial thought (seed)")

    # core engine
    p.add_argument("--max-depth", type=int, default=10)
    p.add_argument("--branches-per-level", type=int, default=4)
    p.add_argument("--keep-top-k", type=int, default=2)
    p.add_argument("--emergent-every", type=int, default=3)
    p.add_argument("--branch-cap", type=int, default=DEFAULT_BRANCH_CAP)
    p.add_argument("--seed-rng", type=int, default=None)

    # memory
    p.add_argument("--load-mem", type=str, default=None)
    p.add_argument("--save-mem", type=str, default=None)
    p.add_argument("--mem-trail", type=str, default=None)
    p.add_argument("--save-trail-name", type=str, default=None)
    p.add_argument("--save-trail-type", type=str, default="integrate")
    p.add_argument("--continue-latest", type=str, default=None)
    p.add_argument("--new-version", action="store_true")
    p.add_argument("--version-comment", type=str, default=None)
    p.add_argument("--max-saved-trails", type=int, default=20)

    # fork + merge trails
    p.add_argument("--fork-trail", type=str, default=None)
    p.add_argument("--fork-from-depth", type=int, default=None)
    p.add_argument("--fork-from-thought", type=str, default=None)
    p.add_argument("--fork-name", type=str, default=None)

    p.add_argument("--merge-trails", type=str, default=None, help="Comma-separated trail names")
    p.add_argument("--merge-name", type=str, default=None)

    # branch merge (inside runtime)
    p.add_argument("--merge-ids", type=str, default=None, help="Comma-separated indices/ranges/substrings")
    p.add_argument("--merge-top-n", type=int, default=None)
    p.add_argument("--merge-top-min-conf", type=float, default=0.0)
    p.add_argument("--merge-all-above-conf", type=float, default=None)
    p.add_argument("--merge-all-above-min-size", type=int, default=2)
    p.add_argument("--merge-emergent-only", action="store_true")
    p.add_argument("--merge-emergent-min-count", type=int, default=2)
    p.add_argument("--merge-depth-range", type=str, default=None, help="e.g. 2-6")
    p.add_argument("--merge-depth-min-count", type=int, default=2)
    p.add_argument("--merge-conflicting", action="store_true")
    p.add_argument("--merge-conflicting-min-conf", type=float, default=0.40)
    p.add_argument("--merge-conflicting-max-pairs", type=int, default=5)
    p.add_argument("--merge-reducer", type=str, default=None, choices=list(REDUCERS.keys()))

    # rag + graph + output
    p.add_argument("--rag-dir", type=str, default=None)
    p.add_argument("--export-graph", type=str, default=None)
    p.add_argument("--json", action="store_true")
    p.add_argument("--full-audit", action="store_true")
    p.add_argument("--audit-per-depth", action="store_true")

    args = p.parse_args()

    if not args.seed and not args.mem_trail and not args.continue_latest and not args.fork_trail and not args.merge_trails:
        p.error("Provide a seed or a resume option (--mem-trail/--continue-latest/--fork-trail/--merge-trails).")

    merge_trails_list = args.merge_trails.split(",") if args.merge_trails else None
    merge_ids_list = [s.strip() for s in args.merge_ids.split(",")] if args.merge_ids else None

    result = agent_think_with_branches(
        initial_thought=args.seed or "",
        max_depth=args.max_depth,
        branches_per_level=args.branches_per_level,
        keep_top_k=args.keep_top_k,
        emergent_every_n_depths=args.emergent_every,
        branch_cap=args.branch_cap,
        random_seed=args.seed_rng,
        return_full_audit=args.full_audit,
        audit_per_depth=args.audit_per_depth,
        load_mem=args.load_mem,
        save_mem=args.save_mem,
        rag_dir=args.rag_dir,
        export_graph_path=args.export_graph,
        mem_trail=args.mem_trail,
        save_trail_name=args.save_trail_name,
        save_trail_type=args.save_trail_type,
        merge_trails=merge_trails_list,
        merge_name=args.merge_name,
        fork_trail_name=args.fork_trail,
        fork_from_depth=args.fork_from_depth,
        fork_from_thought=args.fork_from_thought,
        fork_name=args.fork_name,
        merge_branches_ids=merge_ids_list,
        merge_top_n=args.merge_top_n,
        merge_top_min_conf=args.merge_top_min_conf,
        merge_all_above_conf=args.merge_all_above_conf,
        merge_all_above_min_size=args.merge_all_above_min_size,
        merge_emergent_only=args.merge_emergent_only,
        merge_emergent_min_count=args.merge_emergent_min_count,
        merge_depth_range=args.merge_depth_range,
        merge_depth_min_count=args.merge_depth_min_count,
        merge_conflicting=args.merge_conflicting,
        merge_conflicting_min_conf=args.merge_conflicting_min_conf,
        merge_conflicting_max_pairs=args.merge_conflicting_max_pairs,
        merge_reducer_name=args.merge_reducer,
        continue_latest=args.continue_latest,
        new_version=args.new_version,
        version_comment=args.version_comment,
        max_saved_trails=args.max_saved_trails,
    )

    if args.json:
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("â•" * 80)
        print("Best thought:\n", result["best_thought"])
        print("\nConfidence:", result["best_confidence"])
        print("Stopped:", result["reason_stopped"])
        print("Symbolic:", result.get("symbolic_summary", ""))
        print("Health:", result["health_metrics"])
        print("Last anchor:", result.get("last_anchor", "(none)"))
        if "merged_branch" in result:
            print("\nMerged branch:\n", result["merged_branch"]["thought"])
        print("\nTop branches:")
        for i, b in enumerate(result.get("top_branches", []), 1):
            print(f"  {i}. [{b['confidence']:.3f} d{b['depth']}/l{b['layer']}] {b['thought']}")

if __name__ == "__main__":
    main()
